{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Profile_scraping_ravi_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hetmjnkO2AM"
      },
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acnsfk1dO2AN"
      },
      "source": [
        "url = 'https://patient.info/forums/discuss/browse/genital-herpes-simplex-979'\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMTzNAJnO2AO"
      },
      "source": [
        "r = requests.get(url, headers = headers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGMYo2OFO2AO"
      },
      "source": [
        "soup = BeautifulSoup(r.text, 'html.parser')\n",
        "threads = soup.find_all('div',{'class':'avt avt-sm'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfWQHh0bO2AP"
      },
      "source": [
        "full_list = []\n",
        "next_page = soup.find_all('div',{'class':'group-discussions__paginate_bottom'})[0].find('option').text\n",
        "page_count = int(next_page.split('/')[1])\n",
        "print(page_count)\n",
        "# groups_text_list\n",
        "\n",
        "\n",
        "for page_cnt in range(page_count):\n",
        "  print(page_cnt)\n",
        "  main_url= 'https://patient.info/forums/discuss/browse/genital-herpes-simplex-979'\n",
        "  current_url = main_url+\"?page=%d\" %(page_cnt)+'#group-discussions'\n",
        "  print(current_url)\n",
        "  r_current_url = requests.get(current_url, headers= headers)\n",
        "  current_url_soup = BeautifulSoup(r_current_url.text, 'html.parser')\n",
        "  current_url_threads = current_url_soup.find_all('div',{'class':'avt avt-sm'})\n",
        "    \n",
        "  for item in current_url_threads:\n",
        "    temp_list=[]\n",
        "    ## Extracting profile names\n",
        "    profile_name=item.find('a',{'aria-label':'profile'}).text\n",
        "    profile_name_cleaned = profile_name.replace('\\r\\n','').replace(\" \",\"\")\n",
        "        #     thread_starter_name_list.append(profile_name_cleaned)\n",
        "    print(profile_name_cleaned)\n",
        "        ## Extracting urls of user_names\n",
        "    profile_url = item.find('a',{'aria-label':'profile'})['href']\n",
        "    profile_url_complete = \"https://patient.info\" + profile_url\n",
        "        #     profile_url_list.append(profile_url_complete)\n",
        "        \n",
        "        #identifying groups in which a member is enrolled\n",
        "\n",
        "    ###date the user joined:\n",
        "    r0 = requests.get(profile_url_complete, headers= headers)\n",
        "    soup0 = BeautifulSoup(r0.text, 'html.parser')\n",
        "    \n",
        "    join_date = soup0.find_all('span',{'class':'label masthead__actions__link'})[0].text.replace\\\n",
        "    ('\\n\\n\\n\\n\\n\\r\\n                ', '').replace('\\r\\n            ','')\n",
        "    cleaned_join_date= join_date.split(' ')[1]\n",
        "    print('cleaned_join_date',cleaned_join_date)\n",
        "\n",
        "    ###total_posts by the user till date\n",
        "    total_posts = soup0.find_all('span',{'class':'label masthead__actions__link'})[1].text.replace('\\n\\n\\n\\n\\n\\r\\n                ', '').replace('\\r\\n            ','')\n",
        "    print('total_posts ', total_posts)    \n",
        "\n",
        "    ##view_all_groups url\n",
        "    view_all_groups_url = profile_url_complete + '/groups'\n",
        "    r1 = requests.get(view_all_groups_url, headers= headers)\n",
        "    soup1 = BeautifulSoup(r1.text, 'html.parser')\n",
        "    groups_text= soup1.find_all('h3',{'class':'title'})\n",
        "        \n",
        "        #extracting names of groups to a list\n",
        "    groups_text_list=[]\n",
        "    for item_len in range(len(groups_text)):\n",
        "      try:\n",
        "        groups_text_profile= soup1.find_all('h3',{'class':'title'})[item_len].text#parameterize on i\n",
        "        groups_text_list.append(groups_text_profile)\n",
        "      except Exception as e:\n",
        "        groups_text_profile = 'Content Not Available'\n",
        "        groups_text_list.append(groups_text_profile)\n",
        "            \n",
        "\n",
        "    #extracting all_replies_url page  \n",
        "    view_all_replies_url = profile_url_complete+ '/replies'\n",
        "    r2 = requests.get(view_all_replies_url, headers= headers)\n",
        "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
        "    all_replies= soup2.find_all('div',{'class':'col col--small-12 recent-list'})#[i].text#parameterize on i\n",
        "        \n",
        "    #total_count_of_available_replies\n",
        "    total_count_of_available_replies= len(all_replies)\n",
        "    print('total_count_of_available_replies', total_count_of_available_replies)\n",
        "    \n",
        "    \n",
        "    \n",
        "        \n",
        "    temp_list=[profile_name_cleaned, profile_url_complete, view_all_groups_url, view_all_replies_url, groups_text_list, cleaned_join_date, total_posts, total_count_of_available_replies]\n",
        "    full_list.append(temp_list)\n",
        "        \n",
        "df = pd.DataFrame(full_list,columns=(\"Profile Name\",\"Profile URL\", \"Part of Groups URL\",\"View All Replies URL\",\"Is part of groups\", \"Joined Date\", \"Total_Posts_till_date\", \"available_replies\"))\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "pd.set_option(\"max_colwidth\",None)\n",
        "df\n",
        "#df_final = df[df['profile Name'] == df['author_info']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC0_8Wlsxzu1"
      },
      "source": [
        "df.to_csv('profile_extract_part1.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wknvePVHAeUc"
      },
      "source": [
        "\n",
        "full_final_list=[]\n",
        "for index, row in df.iterrows():\n",
        "    temp_list1 =[]\n",
        "    \n",
        "\n",
        "    if int(row['available_replies'])==0:\n",
        "      \n",
        "      reply_url= 'NA'\n",
        "      response_text= 'NA'\n",
        "      response_time='NA'\n",
        "      temp_list1=[row[\"Profile Name\"], row[\"Profile URL\"], row[\"Part of Groups URL\"], row[\"View All Replies URL\"], row[\"Is part of groups\"], row[\"Joined Date\"], row[\"Total_Posts_till_date\"], row[\"available_replies\"], reply_url, response_text,response_time]\n",
        "      full_final_list.append(temp_list1)\n",
        "\n",
        "    else:\n",
        "      \n",
        "      headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'}\n",
        "      Replies_URL= row['View All Replies URL']\n",
        "      print('Replies_URL: ', Replies_URL)\n",
        "      r = requests.get(Replies_URL, headers= headers)\n",
        "      soup = BeautifulSoup(r.text, 'html.parser')\n",
        "      \n",
        "      response_count= soup.find_all('time',{'class': 'fuzzy'})\n",
        "      print(\"response_count: \", response_count)\n",
        "      for i in range(len(response_count)):\n",
        "        reply_link= soup.find_all('div',{'class':'col col--small-12 recent-list'})[i].find_all('a')[1]['href']\n",
        "        reply_url= 'https://patient.info'+reply_link\n",
        "        print(reply_url)\n",
        "\n",
        "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Safari/537.36'}\n",
        "        r = requests.get(reply_url, headers= headers)\n",
        "        soup5 = BeautifulSoup(r.text, 'html.parser')\n",
        "\n",
        "        print(\"i: \", i)\n",
        "        lower_case_url= str.lower(reply_url)\n",
        "        comment_id= lower_case_url.split('commentid=')[1]\n",
        "        print('comment_id: ', comment_id)\n",
        "        try:\n",
        "          reply_text_1= soup5.find('article',{'id':comment_id}).find('input',{'class':'moderation-conent'})['value']\n",
        "          reply_time_1= soup5.find('article',{'id':comment_id}).find('time',{'class':'fuzzy'})['datetime']\n",
        "        except Exception as e:\n",
        "          reply_text_1 = \"NA\"\n",
        "          reply_time_1 = 'NA'\n",
        "        # reply_text_1= soup5.find('article',{'id':comment_id}).find('input',{'class':'moderation-conent'})['value']\n",
        "        response_text= reply_text_1\n",
        "        response_time= reply_time_1\n",
        "\n",
        "        temp_list1=[row[\"Profile Name\"], row[\"Profile URL\"], row[\"Part of Groups URL\"], row[\"View All Replies URL\"], row[\"Is part of groups\"], row[\"Joined Date\"], row[\"Total_Posts_till_date\"], row[\"available_replies\"], reply_url, response_time, response_text]\n",
        "        full_final_list.append(temp_list1)\n",
        "\n",
        "\n",
        "df_final = pd.DataFrame(full_final_list,columns=(\"Member_Username\", \"Profile URL\", \"Part of Groups URL\", \"View All Replies URL\", \"Is part of groups\", \"Joined Date\", \"Total_Posts_till_date\", \"available_replies_to_extract\", \"specific_reply_url\", \"response_time\",\"response_text\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1nfwWKUM9WJ"
      },
      "source": [
        "df_final.to_csv('profile_extract_finalized.csv', index= False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}